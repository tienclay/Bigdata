{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01737e58-0406-4caf-85c6-c1e44bd5fe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hadoop_Spark_Hive_Integration\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the Spark session\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b95308-9028-4eb1-85cd-8c9235956c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List databases in Hive\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Switch to the default Hive database\n",
    "spark.sql(\"USE default\")\n",
    "\n",
    "# List tables in the database\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55c7ae4-734e-41ed-a741-c1e244a1097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|      Hello World|\n",
      "|  This is a test.|\n",
      "|Spark is awesome!|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create some \"Hello World\" data as an RDD\n",
    "hello_data = [\n",
    "    (\"Hello World\",),\n",
    "    (\"This is a test.\",),\n",
    "    (\"Spark is awesome!\",)\n",
    "]\n",
    "\n",
    "# Create a Spark DataFrame from the RDD\n",
    "df = spark.createDataFrame(hello_data, [\"value\"])\n",
    "\n",
    "# Show the DataFrame contents\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23b0eab-4076-40a1-b6fa-06cb0b4a40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to HDFS\n",
    "df.write.text(\"hdfs://namenode:9000/user/hadoop/hello_world.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9e3f04-dfbf-4ba4-98e6-aeb52a95da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|Spark is awesome!|\n",
      "|  This is a test.|\n",
      "|      Hello World|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the text data back from HDFS\n",
    "df_from_hdfs = spark.read.text(\"hdfs://namenode:9000/user/hadoop/hello_world.txt\")\n",
    "\n",
    "# Show the contents of the file\n",
    "df_from_hdfs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e75189-ea5c-4ebe-87ed-361279547136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|            value|line_length|\n",
      "+-----------------+-----------+\n",
      "|Spark is awesome!|         17|\n",
      "|  This is a test.|         15|\n",
      "|      Hello World|         11|\n",
      "+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Add a column with the length of each line\n",
    "df_transformed = df_from_hdfs.withColumn(\"line_length\", length(df_from_hdfs['value']))\n",
    "\n",
    "# Show the transformed data\n",
    "df_transformed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f16c25-ddc8-41fc-a513-ffd13cda636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|  default|hello_world_table|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the transformed data to a Hive table\n",
    "df_transformed.write.mode(\"overwrite\").saveAsTable(\"hello_world_table\")\n",
    "\n",
    "# Verify if the table was created successfully\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd656624-a1d1-4560-9faf-66bb2553b101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Spark is awesome! 17|\n",
      "|  This is a test. 15|\n",
      "|      Hello World 11|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Concatenate 'value' and 'line_length' columns into a single column\n",
    "df_single_column = df_transformed.withColumn(\"output\", concat_ws(\" \", df_transformed[\"value\"], df_transformed[\"line_length\"]))\n",
    "\n",
    "# Write the single concatenated column to HDFS\n",
    "df_single_column.select(\"output\").write.text(\"hdfs://namenode:9000/user/hadoop/output_hello_world.txt\")\n",
    "\n",
    "# Read the data from HDFS\n",
    "output_df = spark.read.text(\"hdfs://namenode:9000/user/hadoop/output_hello_world.txt\")\n",
    "output_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63567211-6a47-452f-8bd6-d8ae86d9f1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
